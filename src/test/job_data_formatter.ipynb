{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:///test_data/hr_bank_data_db.db'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_INI_DIR = '../../config.ini'\n",
    "\n",
    "# config.ini\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_INI_DIR)\n",
    "\n",
    "# config.ini setting\n",
    "log_folder = config['setting']['log_folder']\n",
    "db_folder = config['setting']['db_folder']\n",
    "db_name = config['setting']['hr_bank_db_name']\n",
    "\n",
    "DB_CONNECTION_STRING = f'sqlite:///test_data/hr_bank_data_db.db'\n",
    "DB_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "# from scrapy.utils.log import configure_logging\n",
    "import logging\n",
    "log_filepath = os.path.join('/Users/yichen/Desktop/hr_bank/log', 'test_hr_bank_crawler.log')\n",
    "logHandler = TimedRotatingFileHandler(log_filepath, when='midnight', interval=10)\n",
    "# configure_logging(install_root_handler=False)\n",
    "logging.basicConfig(handlers=[logHandler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from sqlalchemy import and_\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from model.model import db_connect, create_table, TRawSearch, TRawJob, TRawJobAnalysis, TRawComp, TJob, TJobAnalysis, TComp\n",
    "\n",
    "\n",
    "DOMAIN_URL = 'https://www.104.com.tw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode_blank(text):\n",
    "    text = text.replace('\\xa0', '')\n",
    "    text = text.replace('\\u3000', '')\n",
    "    text = text.replace('\\r', '')\n",
    "    return text\n",
    "\n",
    "    \n",
    "def join_dict_item_in_list(l: list, key: str):\n",
    "    ''' input:  [{\"name\": \"Ian\"}, {\"name\": \"Wang\"}]\n",
    "        output: \"Ian、Wang\"\n",
    "    '''\n",
    "    dict_item_list = [d[key] for d in l]\n",
    "    return '、'.join(dict_item_list)\n",
    "\n",
    "\n",
    "def join_element_in_list(l: list) -> str:\n",
    "    ''' input:  [\"A\", \"B\", \"C\"]\n",
    "        output: \"A、B、C\"\n",
    "    '''\n",
    "    return '、'.join(l)\n",
    "\n",
    "\n",
    "def clean_json(j: str) -> str:\n",
    "    ''' convert full-width character to half-width one, such as \"Ａ\" to \"A\"\n",
    "    '''\n",
    "    j = unicodedata.normalize('NFKC', j)\n",
    "    j = remove_unicode_blank(j)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_analysis_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    for type_, data in crawled_dict.items():\n",
    "        \n",
    "        # 資料更新時間\n",
    "        if 'update_time' not in crawled_dict.keys():\n",
    "            update_time = ''.join(re.findall('\\d+', crawled_dict[type_]['update_time'][:10]))\n",
    "            parsed_job['update_time'] = int(update_time)\n",
    "        \n",
    "        # 應徵人數\n",
    "        if 'apply_count' not in crawled_dict.keys():\n",
    "            parsed_job['apply_count'] = int(crawled_dict[type_]['total'])\n",
    "    \n",
    "    # 應徵人數資料整理\n",
    "    apply_count_dict = _convert_analysis_json(crawled_dict)\n",
    "    \n",
    "    # 性別\n",
    "    parsed_job['sex_m'] = apply_count_dict['sex']['男']\n",
    "    parsed_job['sex_f'] = apply_count_dict['sex']['女']\n",
    "    \n",
    "    # 學歷\n",
    "    parsed_job['edu_na'] = apply_count_dict['edu']['無法判斷']\n",
    "    parsed_job['edu_no'] = apply_count_dict['edu']['不拘']\n",
    "    parsed_job['edu_jr'] = apply_count_dict['edu']['國中(含)以下']\n",
    "    parsed_job['edu_sr'] = apply_count_dict['edu']['高中職']\n",
    "    parsed_job['edu_jr_coll'] = apply_count_dict['edu']['專科']\n",
    "    parsed_job['edu_undergrad'] = apply_count_dict['edu']['大學']\n",
    "    parsed_job['edu_grad'] = apply_count_dict['edu']['博碩士']\n",
    "\n",
    "    # 年齡\n",
    "    parsed_job['age_00_20'] = apply_count_dict['age']['20歲以下']\n",
    "    parsed_job['age_21_25'] = apply_count_dict['age']['21~25歲']\n",
    "    parsed_job['age_26_30'] = apply_count_dict['age']['26~30歲']\n",
    "    parsed_job['age_31_35'] = apply_count_dict['age']['31~35歲']\n",
    "    parsed_job['age_36_40'] = apply_count_dict['age']['36~40歲']\n",
    "    parsed_job['age_41_45'] = apply_count_dict['age']['41~45歲']\n",
    "    parsed_job['age_46_50'] = apply_count_dict['age']['46~50歲']\n",
    "    parsed_job['age_51_55'] = apply_count_dict['age']['51~55歲']\n",
    "    parsed_job['age_56_60'] = apply_count_dict['age']['56~60歲']\n",
    "    parsed_job['age_60_99'] = apply_count_dict['age']['60歲以上']\n",
    "    \n",
    "    # 工作經驗\n",
    "    parsed_job['work_exp_no'] = apply_count_dict['work_exp']['無工作經驗']\n",
    "    parsed_job['work_exp_00_01'] = apply_count_dict['work_exp']['1年以下']\n",
    "    parsed_job['work_exp_01_03'] = apply_count_dict['work_exp']['1~3年']\n",
    "    parsed_job['work_exp_03_05'] = apply_count_dict['work_exp']['3~5年']\n",
    "    parsed_job['work_exp_05_10'] = apply_count_dict['work_exp']['5~10年']\n",
    "    parsed_job['work_exp_10_15'] = apply_count_dict['work_exp']['10~15年']\n",
    "    parsed_job['work_exp_15_20'] = apply_count_dict['work_exp']['15~20年']\n",
    "    parsed_job['work_exp_20_25'] = apply_count_dict['work_exp']['20~25年']\n",
    "    parsed_job['work_exp_25_99'] = apply_count_dict['work_exp']['25年以上']\n",
    "\n",
    "    # 語言\n",
    "    filter_list = ['英文', '中文', '日文', '韓文']\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('英文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('中文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('日文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('韓文', 0)\n",
    "    parsed_job['lang'] = _get_other_apply_count(apply_count_dict['lang'], filter_list)\n",
    "    \n",
    "    # 科系\n",
    "    filter_list = ['資訊管理相關', '資訊工程相關', '統計學相關', '數理統計相關']\n",
    "    parsed_job['major_info_mgmt'] = apply_count_dict['major'].get('資訊管理相關', 0)\n",
    "    parsed_job['major_cs'] = apply_count_dict['major'].get('資訊工程相關', 0)\n",
    "    parsed_job['major_stat'] = apply_count_dict['major'].get('統計學相關', 0)\n",
    "    parsed_job['major_math_stat'] = apply_count_dict['major'].get('數理統計相關', 0)\n",
    "    parsed_job['major_other'] = _get_other_apply_count(apply_count_dict['major'], filter_list)\n",
    "\n",
    "    # 技能\n",
    "    filter_list = ['Java', 'Python']\n",
    "    parsed_job['skill_java'] = apply_count_dict['skill'].get('Java', 0)\n",
    "    parsed_job['skill_python'] = apply_count_dict['skill'].get('Python', 0)\n",
    "    parsed_job['skill_other'] = _get_other_apply_count(apply_count_dict['skill'], filter_list)\n",
    "    \n",
    "    # 證照\n",
    "    filter_list = ['國際專案管理師PMP']\n",
    "    parsed_job['cert_pmp'] = apply_count_dict['cert'].get('國際專案管理師PMP', 0)\n",
    "    parsed_job['cert_other'] = _get_other_apply_count(apply_count_dict['cert'], filter_list)\n",
    "\n",
    "    return parsed_job\n",
    "    \n",
    "\n",
    "def _convert_analysis_json(analysis_dict: dict) -> dict:\n",
    "    type_list = ['sex', 'edu', 'yearRange', 'exp', 'language', 'major', 'skill', 'cert']\n",
    "    new_analysis_dict = {}\n",
    "\n",
    "    for type_ in type_list:\n",
    "\n",
    "        if type_ == 'sex':\n",
    "            new_key = 'sex'\n",
    "\n",
    "        elif type_ == 'edu':\n",
    "            new_key = 'edu'\n",
    "\n",
    "        elif type_ == 'yearRange':\n",
    "            new_key = 'age'\n",
    "\n",
    "        elif type_ == 'exp':\n",
    "            new_key = 'work_exp'\n",
    "\n",
    "        elif type_ == 'language':\n",
    "            new_key = 'lang'\n",
    "\n",
    "        elif type_ == 'major':\n",
    "            new_key = 'major'\n",
    "\n",
    "        elif type_ == 'skill':\n",
    "            new_key = 'skill'\n",
    "\n",
    "        elif type_ == 'cert':\n",
    "            new_key = 'cert'\n",
    "            \n",
    "        else:\n",
    "            new_key = type_\n",
    "\n",
    "        new_analysis_dict[new_key] = {}\n",
    "        for k1, v1 in analysis_dict[type_].items():\n",
    "            if k1.isdigit():\n",
    "                for k2, v2 in v1.items():\n",
    "                    if 'Name' in k2:\n",
    "                        new_analysis_dict[new_key][v2.strip()] = int(v1['count'])\n",
    "                        break\n",
    "                        \n",
    "    return new_analysis_dict\n",
    "\n",
    "\n",
    "def _get_other_apply_count(apply_count: dict, filter_list: list) -> int:\n",
    "    other_count = 0\n",
    "    for k, v in apply_count.items():\n",
    "        if k in filter_list:\n",
    "            continue\n",
    "        other_count += v\n",
    "    return other_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_job_list_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    # 職缺類型\n",
    "    parsed_job['job_type'] = crawled_dict['jobType']\n",
    "    \n",
    "    # 職缺編號\n",
    "    parsed_job['job_no'] = crawled_dict['jobNo']\n",
    "    \n",
    "    # 職缺名稱\n",
    "    parsed_job['job_name'] = crawled_dict['jobName']\n",
    "    \n",
    "    # 工作性質：全職、兼職...\n",
    "    parsed_job['job_role'] = crawled_dict['jobRole']\n",
    "    \n",
    "    # 工作地區：台北市信義區...\n",
    "    parsed_job['job_addr_dist'] = crawled_dict['jobAddrNoDesc']\n",
    "    \n",
    "    # 職缺內容\n",
    "    parsed_job['job_detail'] = crawled_dict['description']\n",
    "    \n",
    "    # 學歷要求\n",
    "    parsed_job['edu'] = crawled_dict['optionEdu']\n",
    "    \n",
    "    # 工作經驗要求\n",
    "    parsed_job['work_exp'] = int(crawled_dict['period'])\n",
    "    \n",
    "    # 應徵人數\n",
    "    parsed_job['apply_count'] = int(crawled_dict['applyCnt'])\n",
    "    \n",
    "    # 公司編號\n",
    "    parsed_job['comp_no'] = crawled_dict['custNo']\n",
    "    \n",
    "    # 公司名稱\n",
    "    parsed_job['comp_name'] = crawled_dict['custName']\n",
    "    \n",
    "    # 產業編號\n",
    "    parsed_job['indust_no'] = crawled_dict['coIndustry']\n",
    "    \n",
    "    # 產業描述\n",
    "    parsed_job['indust_desc'] = crawled_dict['coIndustryDesc']\n",
    "    \n",
    "    # 最低薪資\n",
    "    parsed_job['salary_min'] = int(crawled_dict['salaryLow'])\n",
    "    \n",
    "    # 最高薪資\n",
    "    parsed_job['salary_max'] = int(crawled_dict['salaryHigh'])\n",
    "    \n",
    "    # 薪資描述\n",
    "    parsed_job['salary_desc'] = crawled_dict['salaryDesc']\n",
    "    \n",
    "    # 薪資類型\n",
    "    parsed_job['salary_type'] = crawled_dict['s10']\n",
    "    \n",
    "    # 開缺日期\n",
    "    parsed_job['appear_date'] = int(crawled_dict['appearDate'])\n",
    "    \n",
    "    # 職缺標籤\n",
    "    parsed_job['job_tag'] = join_element_in_list(crawled_dict['tags'])\n",
    "    \n",
    "    # 工作地點地標\n",
    "    parsed_job['landmark_tag'] = crawled_dict['landmark']\n",
    "    \n",
    "    # 職缺連結\n",
    "    try:\n",
    "        parsed_job['job_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['job'])\n",
    "    except:\n",
    "        parsed_job['job_url'] = ''\n",
    "    \n",
    "    # 應徵分析連結\n",
    "    try:\n",
    "        parsed_job['analysis_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['applyAnalyze'])\n",
    "    except:\n",
    "        parsed_job['analysis_url'] = ''\n",
    "    \n",
    "    # 公司連結\n",
    "    try:\n",
    "        parsed_job['comp_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['cust'])\n",
    "    except:\n",
    "        parsed_job['comp_url'] = ''\n",
    "    \n",
    "    # 工作地點經度\n",
    "    parsed_job['lon'] = crawled_dict['lon']\n",
    "    \n",
    "    # 工作地點緯度\n",
    "    parsed_job['lat'] = crawled_dict['lat']\n",
    "    \n",
    "    return parsed_job\n",
    "\n",
    "\n",
    "def _get_web_link(arg: str) -> str:\n",
    "    try:\n",
    "        return urljoin(DOMAIN_URL, crawled_dict['link'][arg])\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_job_detail_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    # 職缺名稱\n",
    "    parsed_job['job_name'] = crawled_dict['header']['jobName']\n",
    "    \n",
    "    # 開缺日期\n",
    "#     parsed_job['appear_date'] = int(re.sub('/', '', crawled_dict['header']['appearDate'])) # second choice\n",
    "\n",
    "    # 公司名稱\n",
    "    parsed_job['comp_name'] = crawled_dict['header']['custName']\n",
    "    \n",
    "    # 公司連結\n",
    "    try:\n",
    "        parsed_job['comp_url'] = urljoin(DOMAIN_URL, crawled_dict['header']['custUrl'])\n",
    "    except:\n",
    "        parsed_job['comp_url'] = ''\n",
    "    \n",
    "    # 分析類型\n",
    "    parsed_job['analysis_type'] = crawled_dict['header']['analysisType']\n",
    "    \n",
    "    # 分析連結\n",
    "    try:\n",
    "        parsed_job['analysis_url'] = urljoin(DOMAIN_URL, crawled_dict['header']['analysis_url'])\n",
    "    except:\n",
    "        parsed_job['analysis_url'] = ''\n",
    "        \n",
    "    # 接受身份\n",
    "    parsed_job['accept_role'] = join_dict_item_in_list(\n",
    "        crawled_dict['condition']['acceptRole']['role'], 'description') \n",
    "    \n",
    "    # 婉拒身份\n",
    "    parsed_job['accept_role'] = join_dict_item_in_list(\n",
    "        crawled_dict['condition']['acceptRole']['disRole']['disability'], 'type') \n",
    "    \n",
    "    # 工作經驗要求\n",
    "#     parsed_job['work_exp'] = int(crawled_dict['condition']['workExp']) # second choice\n",
    "    \n",
    "    # 學歷要求\n",
    "#     parsed_job['edu'] = crawled_dict['condition']['edu'] # second choice\n",
    "    \n",
    "    # 科系要求\n",
    "    parsed_job['major'] = join_element_in_list(crawled_dict['condition']['major'])\n",
    "    \n",
    "    # 語言要求\n",
    "    parsed_job['lang'] = join_dict_item_in_list(crawled_dict['condition']['language'], 'language')\n",
    "    \n",
    "    # 地方語言要求\n",
    "    parsed_job['local_lang'] = join_dict_item_in_list(crawled_dict['condition']['localLanguage'], 'language')\n",
    "    \n",
    "    # 工具要求\n",
    "    parsed_job['specialty'] = join_dict_item_in_list(crawled_dict['condition']['specialty'], 'description')\n",
    "    \n",
    "    # 技能要求\n",
    "    parsed_job['skill'] = join_dict_item_in_list(crawled_dict['condition']['skill'], 'description')\n",
    "    \n",
    "    # 證照要求\n",
    "    parsed_job['cert'] = join_element_in_list(crawled_dict['condition']['certificate'])\n",
    "    \n",
    "    # 駕照要求\n",
    "#     parsed_job['driver_license'] = join_element_in_list(crawled_dict['condition']['driverLicense'])\n",
    "    \n",
    "    # 其他要求\n",
    "    parsed_job['other'] = crawled_dict['condition']['other']\n",
    "    \n",
    "    # 職缺內容\n",
    "    parsed_job['job_detail'] = crawled_dict['jobDetail']['jobDescription']\n",
    "    \n",
    "    # 職缺分類\n",
    "    parsed_job['job_cat'] = join_dict_item_in_list(crawled_dict['jobDetail']['jobCategory'], 'description')\n",
    "    \n",
    "    # 最低薪資\n",
    "    parsed_job['salary_min'] = int(crawled_dict['jobDetail']['salaryMin'])\n",
    "    \n",
    "    # 最高薪資\n",
    "    parsed_job['salary_max'] = int(crawled_dict['jobDetail']['salaryMax'])\n",
    "    \n",
    "    # 薪資描述\n",
    "    parsed_job['salary_desc'] = crawled_dict['jobDetail']['salary']\n",
    "    \n",
    "    # 薪資類型\n",
    "    parsed_job['salary_type'] = crawled_dict['jobDetail']['salaryType']\n",
    "    \n",
    "    # 職缺類型\n",
    "    parsed_job['job_type'] = crawled_dict['jobDetail']['jobType']\n",
    "    \n",
    "    # 工作性質：全職、兼職...\n",
    "    parsed_job['job_role'] = crawled_dict['jobDetail']['workType'] # or work_type?\n",
    "    \n",
    "    # 工作地區：台北市信義區...\n",
    "    parsed_job['job_addr_dist'] = crawled_dict['jobDetail']['addressRegion']\n",
    "    \n",
    "    # 工作地點經度\n",
    "    parsed_job['lon'] = crawled_dict['jobDetail']['longitude']\n",
    "    \n",
    "    # 工作地點緯度\n",
    "    parsed_job['lat'] = crawled_dict['jobDetail']['latitude']\n",
    "    \n",
    "    # 管理責任\n",
    "    parsed_job['manage_resp'] = crawled_dict['jobDetail']['manageResp']\n",
    "    \n",
    "    # 出差外派\n",
    "    parsed_job['business_trip'] = crawled_dict['jobDetail']['businessTrip']\n",
    "    \n",
    "    # 上班時段\n",
    "    parsed_job['work_period'] = crawled_dict['jobDetail']['workPeriod']\n",
    "    \n",
    "    # 休假制度\n",
    "    parsed_job['vacation_policy'] = crawled_dict['jobDetail']['vacationPolicy']\n",
    "    \n",
    "    # 可上班日\n",
    "    parsed_job['start_work_day'] = crawled_dict['jobDetail']['startWorkingDay']\n",
    "    \n",
    "    # 招募類型：0公司自聘、1代徵\n",
    "    parsed_job['hire_type'] = crawled_dict['jobDetail']['hireType']\n",
    "    \n",
    "    # 委託招募單位\n",
    "    parsed_job['delegate_recruit'] = crawled_dict['jobDetail']['delegatedRecruit']\n",
    "    \n",
    "    # 需求人數\n",
    "    parsed_job['need_emp'] = crawled_dict['jobDetail']['needEmp']\n",
    "    \n",
    "    # 最少需求人數\n",
    "    parsed_job['need_count_min'] = _get_need_emp_min(parsed_job['need_emp'])\n",
    "    \n",
    "    # 最少需求人數\n",
    "    parsed_job['need_count_max'] = _get_need_emp_max(parsed_job['need_emp'])\n",
    "    \n",
    "    # 職缺狀況\n",
    "#     parsed_job['switch'] = crawled_dict['switch']['needEmp']\n",
    "    \n",
    "    # 產業描述\n",
    "    parsed_job['indust_desc'] = crawled_dict['industry']\n",
    "    \n",
    "    # 公司編號\n",
    "    parsed_job['comp_no'] = crawled_dict['custNo']\n",
    "    \n",
    "    return parsed_job\n",
    "   \n",
    "    \n",
    "def _get_need_emp_min(need_emp_string):\n",
    "    result = re.findall('(\\d+)', need_emp_string)\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "def _get_need_emp_max(need_emp_string):\n",
    "    result = re.findall('(\\d+)', need_emp_string)\n",
    "    if result:\n",
    "        return result[-1]\n",
    "    else:\n",
    "        return 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DB_CONNECTION_STRING: sqlite:///test_data/hr_bank_data_db.db\n",
      ">>> processing 0 record...\n",
      ">>> processing 0 record... Done!\n",
      ">>> processing 1 record...\n",
      ">>> processing 1 record... Done!\n",
      ">>> processing 2 record...\n",
      ">>> processing 2 record... Done!\n",
      ">>> processing 3 record...\n",
      ">>> processing 3 record... Done!\n",
      ">>> processing 4 record...\n",
      ">>> processing 4 record... Done!\n",
      ">>> processing 5 record...\n",
      ">>> processing 5 record... Done!\n",
      ">>> processing 6 record...\n",
      ">>> processing 6 record... Done!\n",
      ">>> processing 7 record...\n",
      ">>> processing 7 record... Done!\n",
      ">>> processing 8 record...\n",
      ">>> processing 8 record... Done!\n",
      ">>> processing 9 record...\n",
      ">>> processing 9 record... Done!\n",
      ">>> processing 10 record...\n",
      ">>> processing 10 record... Done!\n",
      ">>> processing 11 record...\n",
      ">>> processing 11 record... Done!\n",
      ">>> processing 12 record...\n",
      ">>> processing 12 record... Done!\n",
      ">>> processing 13 record...\n",
      ">>> processing 13 record... Done!\n",
      ">>> processing 14 record...\n",
      ">>> processing 14 record... Done!\n",
      ">>> processing 15 record...\n",
      ">>> processing 15 record... Done!\n",
      ">>> processing 16 record...\n",
      ">>> processing 16 record... Done!\n",
      ">>> processing 17 record...\n",
      ">>> processing 17 record... Done!\n",
      ">>> processing 18 record...\n",
      ">>> processing 18 record... Done!\n",
      ">>> processing 19 record...\n",
      ">>> processing 19 record... Done!\n",
      ">>> processing 20 record...\n",
      ">>> processing 20 record... Done!\n",
      ">>> processing 21 record...\n",
      ">>> processing 21 record... Done!\n",
      ">>> processing 22 record...\n",
      ">>> processing 22 record... Done!\n",
      ">>> processing 23 record...\n",
      ">>> processing 23 record... Done!\n",
      ">>> processing 24 record...\n",
      ">>> processing 24 record... Done!\n",
      ">>> processing 25 record...\n",
      ">>> processing 25 record... Done!\n",
      ">>> processing 26 record...\n",
      ">>> processing 26 record... Done!\n",
      ">>> processing 27 record...\n",
      ">>> processing 27 record... Done!\n",
      ">>> processing 28 record...\n",
      ">>> processing 28 record... Done!\n",
      ">>> processing 29 record...\n",
      ">>> processing 29 record... Done!\n",
      ">>> processing 30 record...\n",
      ">>> processing 30 record... Done!\n",
      ">>> processing 31 record...\n",
      ">>> processing 31 record... Done!\n",
      ">>> processing 32 record...\n",
      ">>> processing 32 record... Done!\n",
      ">>> processing 33 record...\n",
      ">>> processing 33 record... Done!\n",
      ">>> processing 34 record...\n",
      ">>> processing 34 record... Done!\n",
      ">>> processing 35 record...\n",
      ">>> processing 35 record... Done!\n",
      ">>> processing 36 record...\n",
      ">>> processing 36 record... Done!\n",
      ">>> processing 37 record...\n",
      ">>> processing 37 record... Done!\n",
      ">>> processing 38 record...\n",
      ">>> processing 38 record... Done!\n",
      ">>> processing 39 record...\n",
      ">>> processing 39 record... Done!\n",
      ">>> processing 40 record...\n",
      ">>> processing 40 record... Done!\n",
      ">>> processing 41 record...\n",
      ">>> processing 41 record... Done!\n",
      ">>> processing 42 record...\n",
      ">>> processing 42 record... Done!\n",
      ">>> processing 43 record...\n",
      ">>> processing 43 record... Done!\n",
      ">>> processing 44 record...\n",
      ">>> processing 44 record... Done!\n",
      ">>> processing 45 record...\n",
      ">>> processing 45 record... Done!\n",
      ">>> processing 46 record...\n",
      ">>> processing 46 record... Done!\n",
      ">>> processing 47 record...\n",
      ">>> processing 47 record... Done!\n",
      ">>> processing 48 record...\n",
      ">>> processing 48 record... Done!\n",
      ">>> processing 49 record...\n",
      ">>> processing 49 record... Done!\n",
      ">>> processing 50 record...\n",
      ">>> processing 50 record... Done!\n",
      ">>> processing 51 record...\n",
      ">>> processing 51 record... Done!\n",
      ">>> processing 52 record...\n",
      ">>> processing 52 record... Done!\n",
      ">>> processing 53 record...\n",
      ">>> processing 53 record... Done!\n",
      ">>> processing 54 record...\n",
      ">>> processing 54 record... Done!\n",
      ">>> processing 55 record...\n",
      ">>> processing 55 record... Done!\n",
      ">>> processing 56 record...\n",
      ">>> processing 56 record... Done!\n",
      ">>> processing 57 record...\n",
      ">>> processing 57 record... Done!\n",
      ">>> processing 58 record...\n",
      ">>> processing 58 record... Done!\n",
      ">>> processing 59 record...\n",
      ">>> processing 59 record... Done!\n",
      ">>> processing 60 record...\n",
      ">>> processing 60 record... Done!\n",
      ">>> processing 61 record...\n",
      ">>> processing 61 record... Done!\n",
      ">>> processing 62 record...\n",
      ">>> processing 62 record... Done!\n",
      ">>> processing 63 record...\n",
      ">>> processing 63 record... Done!\n",
      ">>> processing 64 record...\n",
      ">>> processing 64 record... Done!\n",
      ">>> processing 65 record...\n",
      ">>> processing 65 record... Done!\n",
      ">>> processing 66 record...\n",
      ">>> processing 66 record... Done!\n",
      ">>> processing 67 record...\n",
      ">>> processing 67 record... Done!\n",
      ">>> processing 68 record...\n",
      ">>> processing 68 record... Done!\n",
      ">>> processing 69 record...\n",
      ">>> processing 69 record... Done!\n",
      ">>> processing 70 record...\n",
      ">>> processing 70 record... Done!\n",
      ">>> processing 71 record...\n",
      ">>> processing 71 record... Done!\n",
      ">>> processing 72 record...\n",
      ">>> processing 72 record... Done!\n",
      ">>> processing 73 record...\n",
      ">>> processing 73 record... Done!\n",
      ">>> processing 74 record...\n",
      ">>> processing 74 record... Done!\n",
      ">>> processing 75 record...\n",
      ">>> processing 75 record... Done!\n",
      ">>> processing 76 record...\n",
      ">>> processing 76 record... Done!\n",
      ">>> processing 77 record...\n",
      ">>> processing 77 record... Done!\n",
      ">>> processing 78 record...\n",
      ">>> processing 78 record... Done!\n",
      ">>> processing 79 record...\n",
      ">>> processing 79 record... Done!\n",
      ">>> processing 80 record...\n",
      ">>> processing 80 record... Done!\n",
      ">>> processing 81 record...\n",
      ">>> processing 81 record... Done!\n",
      ">>> processing 82 record...\n",
      ">>> processing 82 record... Done!\n",
      ">>> processing 83 record...\n",
      ">>> processing 83 record... Done!\n",
      ">>> processing 84 record...\n",
      ">>> processing 84 record... Done!\n",
      ">>> processing 85 record...\n",
      ">>> processing 85 record... Done!\n",
      ">>> processing 86 record...\n",
      ">>> processing 86 record... Done!\n",
      ">>> processing 87 record...\n",
      ">>> processing 87 record... Done!\n",
      ">>> processing 88 record...\n",
      ">>> processing 88 record... Done!\n",
      ">>> processing 89 record...\n",
      ">>> processing 89 record... Done!\n",
      ">>> processing 90 record...\n",
      ">>> processing 90 record... Done!\n",
      ">>> processing 91 record...\n",
      ">>> processing 91 record... Done!\n",
      ">>> processing 92 record...\n",
      ">>> processing 92 record... Done!\n",
      ">>> processing 93 record...\n",
      ">>> processing 93 record... Done!\n",
      ">>> processing 94 record...\n",
      ">>> processing 94 record... Done!\n",
      ">>> processing 95 record...\n",
      ">>> processing 95 record... Done!\n",
      ">>> processing 96 record...\n",
      ">>> processing 96 record... Done!\n",
      ">>> processing 97 record...\n",
      ">>> processing 97 record... Done!\n",
      ">>> processing 98 record...\n",
      ">>> processing 98 record... Done!\n",
      ">>> processing 99 record...\n",
      ">>> processing 99 record... Done!\n",
      ">>> processing 100 record...\n",
      ">>> processing 100 record... Done!\n",
      ">>> processing 101 record...\n",
      ">>> processing 101 record... Done!\n",
      ">>> processing 102 record...\n",
      ">>> processing 102 record... Done!\n",
      ">>> processing 103 record...\n",
      ">>> processing 103 record... Done!\n"
     ]
    }
   ],
   "source": [
    "class JobDataFormatter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        engine = db_connect(DB_CONNECTION_STRING)\n",
    "        create_table(engine)\n",
    "        self.Session = sessionmaker(bind=engine)\n",
    "\n",
    "\n",
    "    def process(self, crawl_date=None):\n",
    "        session = self.Session()\n",
    "        records = []\n",
    "        \n",
    "        try:\n",
    "            if crawl_date:\n",
    "                crawl_date = int(crawl_date)\n",
    "                results = session.query(TRawWearch).filter(crawl_date==crawl_date).all()\n",
    "            else:\n",
    "                results = session.query(TRawSearch).all() # main table\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.error('can not get data from \"TRawSearch\".')\n",
    "            session.close()\n",
    "            return records\n",
    "        \n",
    "        for i, r in enumerate(results):\n",
    "            print(f'>>> processing {i} record...')\n",
    "            job_no = r.job_no\n",
    "            crawl_date = r.crawl_date\n",
    "            logging.info(f'process record: JOB_NO={job_no}, CRAWL_DATE={crawl_date}')\n",
    "            \n",
    "            try:\n",
    "                # job list content\n",
    "                json_string = clean_json(r.json_string)\n",
    "                search_json = json.loads(json_string)\n",
    "                search_json = _parse_job_list_content(search_json)\n",
    "\n",
    "                # job detail\n",
    "                raw_job = session.query(TRawJob).filter(job_no==job_no, crawl_date==crawl_date).first()\n",
    "                if raw_job:\n",
    "                        json_string = clean_json(raw_job.json_string)\n",
    "                        job_json = json.loads(json_string)\n",
    "                        job_json = _parse_job_detail_content(job_json['data'])\n",
    "                else:\n",
    "                    job_json = {}\n",
    "\n",
    "                # analysis\n",
    "                raw_analysis = session.query(TRawJobAnalysis).filter(job_no==job_no, crawl_date==crawl_date).first()\n",
    "                if raw_analysis:\n",
    "                    json_string = clean_json(raw_analysis.json_string)\n",
    "                    analysis_json = json.loads(json_string)\n",
    "                    analysis_json = _parse_analysis_content(analysis_json)\n",
    "                else:\n",
    "                    analysis_json = {}\n",
    "                \n",
    "                # combine as one record\n",
    "                record = {**analysis_json, **job_json, **search_json}\n",
    "                records.append(record)\n",
    "                print(f'>>> processing {i} record... Done!')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                logging.error(f'No. {i} record:\\nJOB_NO={job_no}\\nCRAWL_DATE={crawl_date}\\n{e}')\n",
    "                \n",
    "        session.close()\n",
    "        return records\n",
    "\n",
    "            \n",
    "job_data_formatter = JobDataFormatter()\n",
    "records = job_data_formatter.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_column_name_to_desc_for_export = {\n",
    "    \n",
    "    'appear_date': '開缺日期',\n",
    "    'update_time': '更新日期',\n",
    "    'job_name': '職缺名稱',\n",
    "    'job_no': '職缺編號',\n",
    "    'job_url': '職缺連結',\n",
    "    'comp_name': '公司名稱',\n",
    "    'comp_no': '公司編號',\n",
    "    'comp_url': '公司連結',\n",
    "    'indust_desc': '產業描述',\n",
    "    'indust_no': '產業編號',\n",
    "    \n",
    "    'need_emp': '需求人數',\n",
    "    'need_count_max': '最多需求人數',\n",
    "    'need_count_min': '最少需求人數',\n",
    "\n",
    "    'job_cat': '工作分類',\n",
    "    'job_detail': '職缺內容',\n",
    "    'job_type': '職缺類型',\n",
    "    'job_tag': '職缺標籤',\n",
    "    'job_role': '工作性質',\n",
    "    'salary_desc': '薪資描述',\n",
    "    'salary_max': '最高薪資',\n",
    "    'salary_min': '最低薪資',\n",
    "    'salary_type': '薪資類型',\n",
    "    'job_addr_dist': '工作地區',\n",
    "    'manage_resp': '管理責任',\n",
    "    'business_trip': '出差外派',\n",
    "    'vacation_policy': '休假制度',\n",
    "    \n",
    "    'edu': '學歷要求',\n",
    "    'major': '科系要求',\n",
    "    'work_exp': '工作經驗要求',\n",
    "    'skill': '技能要求',\n",
    "    'specialty': '專長要求',\n",
    "    'lang': '語言要求',\n",
    "    'local_lang': '地方語言要求',\n",
    "    'cert': '證照要求',\n",
    "    'other': '其他要求',\n",
    "    'accept_role': '接受身份',\n",
    "    'start_work_day': '可上班日',\n",
    "    \n",
    "    'apply_count': '應徵人數',\n",
    "    \n",
    "    'analysis_type': '分析類型',\n",
    "    'analysis_url': '分析連結',\n",
    "    'age_00_20': '20歲以下',\n",
    "    'age_21_25': '21-25歲',\n",
    "    'age_26_30': '26-30歲',\n",
    "    'age_31_35': '31-35歲',\n",
    "    'age_36_40': '36-40歲',\n",
    "    'age_41_45': '41-45歲',\n",
    "    'age_46_50': '46-50歲',\n",
    "    'age_51_55': '51-55歲',\n",
    "    'age_56_60': '56-60歲',\n",
    "    'age_60_99': '60歲以上',\n",
    "    \n",
    "    'major_cs': '資訊工程相關',\n",
    "    'major_info_mgmt': '資訊管理相關',\n",
    "    'major_math_stat': '數理統計相關',\n",
    "    'major_stat': '統計學相關',\n",
    "    'major_other': '其他科系',\n",
    "    \n",
    "    'work_exp_00_01': '1年以下',\n",
    "    'work_exp_01_03': '1-3年',\n",
    "    'work_exp_03_05': '3-5年',\n",
    "    'work_exp_05_10': '5-10年',\n",
    "    'work_exp_10_15': '10-15年',\n",
    "    'work_exp_15_20': '15-20年',\n",
    "    'work_exp_20_25': '20-25年',\n",
    "    'work_exp_25_99': '25年以上',\n",
    "    'work_exp_no': '無工作經驗',\n",
    "    'work_period': '上班時段',\n",
    "    \n",
    "    'cert_other': '其他證照',\n",
    "    'cert_pmp': 'PMP專案證照',\n",
    "    \n",
    "    'edu_jr': '國中',\n",
    "    'edu_jr_coll': '專科',\n",
    "    'edu_sr': '高中',\n",
    "    'edu_undergrad': '大學',\n",
    "    'edu_grad': '碩士以上',\n",
    "    'edu_no': '學歷不拘',\n",
    "    'edu_na': '無法判斷',\n",
    "    \n",
    "    'sex_f': '女性',\n",
    "    'sex_m': '男性',\n",
    "    \n",
    "    'skill_java': 'Java',\n",
    "    'skill_python': 'Python',\n",
    "    'skill_other': '其他技能要求',\n",
    "\n",
    "    'hire_type': '招募類型',\n",
    "    'delegate_recruit': '託招募單位',\n",
    "    \n",
    "    'landmark_tag': '工作地點地標',\n",
    "    'lat': '工作地點緯度',\n",
    "    'lon': '工作地點經度',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>開缺日期</th>\n",
       "      <th>更新日期</th>\n",
       "      <th>職缺名稱</th>\n",
       "      <th>職缺編號</th>\n",
       "      <th>職缺連結</th>\n",
       "      <th>公司名稱</th>\n",
       "      <th>公司編號</th>\n",
       "      <th>公司連結</th>\n",
       "      <th>產業描述</th>\n",
       "      <th>產業編號</th>\n",
       "      <th>需求人數</th>\n",
       "      <th>最多需求人數</th>\n",
       "      <th>最少需求人數</th>\n",
       "      <th>工作分類</th>\n",
       "      <th>職缺內容</th>\n",
       "      <th>職缺類型</th>\n",
       "      <th>職缺標籤</th>\n",
       "      <th>工作性質</th>\n",
       "      <th>薪資描述</th>\n",
       "      <th>最高薪資</th>\n",
       "      <th>最低薪資</th>\n",
       "      <th>薪資類型</th>\n",
       "      <th>工作地區</th>\n",
       "      <th>管理責任</th>\n",
       "      <th>出差外派</th>\n",
       "      <th>休假制度</th>\n",
       "      <th>學歷要求</th>\n",
       "      <th>科系要求</th>\n",
       "      <th>工作經驗要求</th>\n",
       "      <th>技能要求</th>\n",
       "      <th>專長要求</th>\n",
       "      <th>語言要求</th>\n",
       "      <th>地方語言要求</th>\n",
       "      <th>證照要求</th>\n",
       "      <th>其他要求</th>\n",
       "      <th>接受身份</th>\n",
       "      <th>可上班日</th>\n",
       "      <th>應徵人數</th>\n",
       "      <th>分析類型</th>\n",
       "      <th>分析連結</th>\n",
       "      <th>20歲以下</th>\n",
       "      <th>21-25歲</th>\n",
       "      <th>26-30歲</th>\n",
       "      <th>31-35歲</th>\n",
       "      <th>36-40歲</th>\n",
       "      <th>41-45歲</th>\n",
       "      <th>46-50歲</th>\n",
       "      <th>51-55歲</th>\n",
       "      <th>56-60歲</th>\n",
       "      <th>60歲以上</th>\n",
       "      <th>資訊工程相關</th>\n",
       "      <th>資訊管理相關</th>\n",
       "      <th>數理統計相關</th>\n",
       "      <th>統計學相關</th>\n",
       "      <th>其他科系</th>\n",
       "      <th>1年以下</th>\n",
       "      <th>1-3年</th>\n",
       "      <th>3-5年</th>\n",
       "      <th>5-10年</th>\n",
       "      <th>10-15年</th>\n",
       "      <th>15-20年</th>\n",
       "      <th>20-25年</th>\n",
       "      <th>25年以上</th>\n",
       "      <th>無工作經驗</th>\n",
       "      <th>上班時段</th>\n",
       "      <th>其他證照</th>\n",
       "      <th>PMP專案證照</th>\n",
       "      <th>國中</th>\n",
       "      <th>專科</th>\n",
       "      <th>高中</th>\n",
       "      <th>大學</th>\n",
       "      <th>碩士以上</th>\n",
       "      <th>學歷不拘</th>\n",
       "      <th>無法判斷</th>\n",
       "      <th>女性</th>\n",
       "      <th>男性</th>\n",
       "      <th>Java</th>\n",
       "      <th>Python</th>\n",
       "      <th>其他技能要求</th>\n",
       "      <th>招募類型</th>\n",
       "      <th>託招募單位</th>\n",
       "      <th>工作地點地標</th>\n",
       "      <th>工作地點緯度</th>\n",
       "      <th>工作地點經度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20201225</td>\n",
       "      <td>20201227</td>\n",
       "      <td>【RD】資安開發工程師- 2000781</td>\n",
       "      <td>12008786</td>\n",
       "      <td>https://www.104.com.tw/job/75e1e?jobsource=n10...</td>\n",
       "      <td>安碁資訊股份有限公司</td>\n",
       "      <td>70565450000</td>\n",
       "      <td>https://www.104.com.tw/company/wf0uhow?jobsour...</td>\n",
       "      <td>網際網路相關業</td>\n",
       "      <td>1001001003</td>\n",
       "      <td>1人</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>產品企劃開發人員、MIS程式設計師、Internet程式設計師</td>\n",
       "      <td>1. 資安產品開發\\n2. 資安服務自動化流程規劃\\n3. 資安弱點研究與漏洞挖掘\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>員工230人</td>\n",
       "      <td>1</td>\n",
       "      <td>月薪35,000元以上</td>\n",
       "      <td>9999999</td>\n",
       "      <td>35000</td>\n",
       "      <td>50</td>\n",
       "      <td>台北市信義區</td>\n",
       "      <td>不需負擔管理責任</td>\n",
       "      <td>無需出差外派</td>\n",
       "      <td>週休二日</td>\n",
       "      <td>大學</td>\n",
       "      <td>資訊管理相關</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1. 大學(含)以上畢業, 至少2年(含)以上工作經驗\\n2. 優秀的學習能力,分析和解決問...</td>\n",
       "      <td></td>\n",
       "      <td>一個月內</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.104.com.tw/jobs/apply/analysis/75e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>日班</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>距捷運永春站410公尺</td>\n",
       "      <td>25.0409201</td>\n",
       "      <td>121.5720055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       開缺日期      更新日期                  職缺名稱      職缺編號  \\\n",
       "0  20201225  20201227  【RD】資安開發工程師- 2000781  12008786   \n",
       "\n",
       "                                                職缺連結        公司名稱         公司編號  \\\n",
       "0  https://www.104.com.tw/job/75e1e?jobsource=n10...  安碁資訊股份有限公司  70565450000   \n",
       "\n",
       "                                                公司連結     產業描述        產業編號  \\\n",
       "0  https://www.104.com.tw/company/wf0uhow?jobsour...  網際網路相關業  1001001003   \n",
       "\n",
       "  需求人數 最多需求人數 最少需求人數                             工作分類  \\\n",
       "0   1人      1      1  產品企劃開發人員、MIS程式設計師、Internet程式設計師   \n",
       "\n",
       "                                          職缺內容 職缺類型    職缺標籤 工作性質         薪資描述  \\\n",
       "0  1. 資安產品開發\\n2. 資安服務自動化流程規劃\\n3. 資安弱點研究與漏洞挖掘\\n    0  員工230人    1  月薪35,000元以上   \n",
       "\n",
       "      最高薪資   最低薪資 薪資類型    工作地區      管理責任    出差外派  休假制度 學歷要求    科系要求  工作經驗要求  \\\n",
       "0  9999999  35000   50  台北市信義區  不需負擔管理責任  無需出差外派  週休二日   大學  資訊管理相關       3   \n",
       "\n",
       "  技能要求 專長要求 語言要求 地方語言要求 證照要求  \\\n",
       "0                              \n",
       "\n",
       "                                                其他要求 接受身份  可上班日  應徵人數  分析類型  \\\n",
       "0  1. 大學(含)以上畢業, 至少2年(含)以上工作經驗\\n2. 優秀的學習能力,分析和解決問...       一個月內     0     1   \n",
       "\n",
       "                                                分析連結  20歲以下  21-25歲  26-30歲  \\\n",
       "0  https://www.104.com.tw/jobs/apply/analysis/75e...      0       0       0   \n",
       "\n",
       "   31-35歲  36-40歲  41-45歲  46-50歲  51-55歲  56-60歲  60歲以上  資訊工程相關  資訊管理相關  \\\n",
       "0       0       0       0       0       0       0      0       0       0   \n",
       "\n",
       "   數理統計相關  統計學相關  其他科系  1年以下  1-3年  3-5年  5-10年  10-15年  15-20年  20-25年  \\\n",
       "0       0      0     0     0     0     0      0       0       0       0   \n",
       "\n",
       "   25年以上  無工作經驗 上班時段  其他證照  PMP專案證照  國中  專科  高中  大學  碩士以上  學歷不拘  無法判斷  女性  男性  \\\n",
       "0      0      0   日班     0        0   0   0   0   0     0     0     0   0   0   \n",
       "\n",
       "   Java  Python  其他技能要求  招募類型 託招募單位       工作地點地標      工作地點緯度       工作地點經度  \n",
       "0     0       0       0     0        距捷運永春站410公尺  25.0409201  121.5720055  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df = df[list(map_column_name_to_desc_for_export.keys())]\n",
    "new_column_name = list(map_column_name_to_desc_for_export.values())\n",
    "df.columns = new_column_name\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = session.query(TRawSearch).all()\n",
    "for i, r in enumerate(result):\n",
    "    print(i)\n",
    "    job_no = r.job_no\n",
    "    crawl_date = r.crawl_date\n",
    "    \n",
    "    # job list content\n",
    "    json_string = clean_json(r.json_string)\n",
    "    search_json = json.loads(json_string)\n",
    "    search_json = _parse_job_list_content(search_json)\n",
    "    \n",
    "    # job detail\n",
    "    raw_job = session.query(TRawJob).filter(job_no==job_no, crawl_date==crawl_date).first()\n",
    "    json_string = clean_json(raw_job.json_string)\n",
    "    job_json = json.loads(json_string)\n",
    "    job_json = _parse_job_detail_content(job_json['data'])\n",
    "    \n",
    "    # analysis\n",
    "    raw_analysis = session.query(TRawJobAnalysis).filter(job_no==job_no, crawl_date==crawl_date).first()\n",
    "    json_string = clean_json(raw_analysis.json_string)\n",
    "    analysis_json = json.loads(json_string)\n",
    "    analysis_json = _parse_analysis_content(analysis_json)\n",
    "    \n",
    "    print({**analysis_json, **job_json, **search_json})\n",
    "    print('\\n\\n')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_string = unicodedata.normalize('NFKC', r.json_string)\n",
    "json_string = remove_unicode_blank(json_string)\n",
    "analysis_dict = json.loads(json_string)\n",
    "for k in analysis_dict.keys():\n",
    "    print(f\"'{k}': '',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_search_json_key = {\n",
    "    'jobType': 'job_type',\n",
    "    'jobNo': 'job_no',\n",
    "    'jobName': 'job_name',\n",
    "    'jobRole': 'job_role',\n",
    "    'jobAddrNoDesc': 'job_addr_dist',\n",
    "    'description': 'job_detail',\n",
    "    'optionEdu': 'edu',\n",
    "    'period': 'work_exp',\n",
    "    'applyCnt': 'apply_count',\n",
    "    'custNo': 'comp_no',\n",
    "    'custName': 'comp_name',\n",
    "    'coIndustry': 'indust_no',\n",
    "    'coIndustryDesc': 'indust_desc',\n",
    "    'salaryLow': 'salary_min',\n",
    "    'salaryHigh': 'salary_max',\n",
    "    'salaryDesc': 'salary_desc',\n",
    "    's10': 'salary_type',\n",
    "    'appearDate': 'appear_date',\n",
    "    'tags': 'job_tag',\n",
    "    'landmark': 'landmark_tag',\n",
    "    'link': 'links',\n",
    "    'lon': 'lon',\n",
    "    'lat': 'lat',\n",
    "}\n",
    "\n",
    "map_job_json_key = {\n",
    "    'jobName': 'job_name',\n",
    "    'appearDate': 'appear_date',\n",
    "    'custName': 'comp_name',\n",
    "    'custUrl': 'comp_url',\n",
    "    'analysisType': 'analysis_type',\n",
    "    'analysisUrl': 'analysis_url',\n",
    "    'acceptRole': 'accept_role',\n",
    "    'disRole': 'disaccept_role',\n",
    "    'workExp': 'work_exp',\n",
    "    'edu': 'edu',\n",
    "    'major': 'major',\n",
    "    'language': 'lang',\n",
    "    'localLanguage': 'local_lang',\n",
    "    'specialty': 'specialty',\n",
    "    'skill': 'skill',\n",
    "    'certificate': 'cert',\n",
    "    'driverLicense': 'driver_license',\n",
    "    'other': 'other',\n",
    "    'legalTag': 'legal_tag',\n",
    "    'jobDescription': 'job_detail',\n",
    "    'jobCategory': 'job_cat',\n",
    "    'salary': 'salary_desc',\n",
    "    'salaryMin': 'salary_min',\n",
    "    'salaryMax': 'salary_max',\n",
    "    'salaryType': 'salary_type',\n",
    "    'jobType': 'job_type',\n",
    "    'workType': 'work_type',\n",
    "    'addressRegion': 'job_addr_dist',\n",
    "    'longitude': 'lon',\n",
    "    'latitude': 'lat',\n",
    "    'manageResp': 'manage_resp',\n",
    "    'businessTrip': 'business_trip',\n",
    "    'workPeriod': 'work_period',\n",
    "    'vacationPolicy': 'vacation_policy',\n",
    "    'startWorkingDay': 'start_work_day',\n",
    "    'hireType': 'hire_type',\n",
    "    'delegatedRecruit': 'delegate_recruit',\n",
    "    'needEmp': 'need_emp',\n",
    "    'switch': 'switch',\n",
    "    'industry': 'indust_desc',\n",
    "    'custNo': 'comp_no',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_analysis_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    for type_, data in crawled_dict.items():\n",
    "        \n",
    "        # 資料更新時間\n",
    "        if 'update_time' not in crawled_dict.keys():\n",
    "            update_time = ''.join(re.findall('\\d+', crawled_dict[type_]['update_time'][:10]))\n",
    "            parsed_job['update_time'] = int(update_time)\n",
    "        \n",
    "        # 應徵人數\n",
    "        if 'apply_count' not in crawled_dict.keys():\n",
    "            parsed_job['apply_count'] = int(crawled_dict[type_]['total'])\n",
    "    \n",
    "    # 應徵人數資料整理\n",
    "    apply_count_dict = _convert_analysis_json(crawled_dict)\n",
    "    \n",
    "    # 性別\n",
    "    parsed_job['sex_m'] = apply_count_dict['sex']['男']\n",
    "    parsed_job['sex_f'] = apply_count_dict['sex']['女']\n",
    "    \n",
    "    # 學歷\n",
    "    parsed_job['edu_na'] = apply_count_dict['edu']['無法判斷']\n",
    "    parsed_job['edu_no'] = apply_count_dict['edu']['不拘']\n",
    "    parsed_job['edu_jr'] = apply_count_dict['edu']['國中(含)以下']\n",
    "    parsed_job['edu_sr'] = apply_count_dict['edu']['高中職']\n",
    "    parsed_job['edu_jr_coll'] = apply_count_dict['edu']['專科']\n",
    "    parsed_job['edu_undergrad'] = apply_count_dict['edu']['大學']\n",
    "    parsed_job['edu_grad'] = apply_count_dict['edu']['博碩士']\n",
    "\n",
    "    # 年齡\n",
    "    parsed_job['age_00_20'] = apply_count_dict['age']['20歲以下']\n",
    "    parsed_job['age_21_25'] = apply_count_dict['age']['21~25歲']\n",
    "    parsed_job['age_26_30'] = apply_count_dict['age']['26~30歲']\n",
    "    parsed_job['age_31_35'] = apply_count_dict['age']['31~35歲']\n",
    "    parsed_job['age_36_40'] = apply_count_dict['age']['36~40歲']\n",
    "    parsed_job['age_41_45'] = apply_count_dict['age']['41~45歲']\n",
    "    parsed_job['age_46_50'] = apply_count_dict['age']['46~50歲']\n",
    "    parsed_job['age_51_55'] = apply_count_dict['age']['51~55歲']\n",
    "    parsed_job['age_56_60'] = apply_count_dict['age']['56~60歲']\n",
    "    parsed_job['age_60_99'] = apply_count_dict['age']['60歲以上']\n",
    "    \n",
    "    # 工作經驗\n",
    "    parsed_job['work_exp_no'] = apply_count_dict['work_exp']['無工作經驗']\n",
    "    parsed_job['work_exp_00_01'] = apply_count_dict['work_exp']['1年以下']\n",
    "    parsed_job['work_exp_01_03'] = apply_count_dict['work_exp']['1~3年']\n",
    "    parsed_job['work_exp_03_05'] = apply_count_dict['work_exp']['3~5年']\n",
    "    parsed_job['work_exp_05_10'] = apply_count_dict['work_exp']['5~10年']\n",
    "    parsed_job['work_exp_10_15'] = apply_count_dict['work_exp']['10~15年']\n",
    "    parsed_job['work_exp_15_20'] = apply_count_dict['work_exp']['15~20年']\n",
    "    parsed_job['work_exp_20_25'] = apply_count_dict['work_exp']['20~25年']\n",
    "    parsed_job['work_exp_25_99'] = apply_count_dict['work_exp']['25年以上']\n",
    "\n",
    "    # 語言\n",
    "    filter_list = ['英文', '中文', '日文', '韓文']\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('英文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('中文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('日文', 0)\n",
    "    parsed_job['lang'] = apply_count_dict['lang'].get('韓文', 0)\n",
    "    parsed_job['lang'] = _get_other_apply_count(apply_count_dict['lang'], filter_list)\n",
    "    \n",
    "    # 科系\n",
    "    filter_list = ['資訊管理相關', '資訊工程相關', '統計學相關', '數理統計相關']\n",
    "    parsed_job['major_info_mgmt'] = apply_count_dict['major'].get('資訊管理相關', 0)\n",
    "    parsed_job['major_cs'] = apply_count_dict['major'].get('資訊工程相關', 0)\n",
    "    parsed_job['major_stat'] = apply_count_dict['major'].get('統計學相關', 0)\n",
    "    parsed_job['major_math_stat'] = apply_count_dict['major'].get('數理統計相關', 0)\n",
    "    parsed_job['major_other'] = _get_other_apply_count(apply_count_dict['major'], filter_list)\n",
    "\n",
    "    # 技能\n",
    "    filter_list = ['Java', 'Python']\n",
    "    parsed_job['skill_java'] = apply_count_dict['skill'].get('Java', 0)\n",
    "    parsed_job['skill_python'] = apply_count_dict['skill'].get('Python', 0)\n",
    "    parsed_job['skill_other'] = _get_other_apply_count(apply_count_dict['skill'], filter_list)\n",
    "    \n",
    "    # 證照\n",
    "    filter_list = ['國際專案管理師PMP']\n",
    "    parsed_job['cert_pmp'] = apply_count_dict['cert'].get('國際專案管理師PMP', 0)\n",
    "    parsed_job['cert_other'] = _get_other_apply_count(apply_count_dict['cert'], filter_list)\n",
    "\n",
    "    return parsed_job\n",
    "    \n",
    "\n",
    "def _convert_analysis_json(analysis_dict: dict) -> dict:\n",
    "    type_list = ['sex', 'edu', 'yearRange', 'exp', 'language', 'major', 'skill', 'cert']\n",
    "    new_analysis_dict = {}\n",
    "\n",
    "    for type_ in type_list:\n",
    "\n",
    "        if type_ == 'sex':\n",
    "            new_key = 'sex'\n",
    "\n",
    "        elif type_ == 'edu':\n",
    "            new_key = 'edu'\n",
    "\n",
    "        elif type_ == 'yearRange':\n",
    "            new_key = 'age'\n",
    "\n",
    "        elif type_ == 'exp':\n",
    "            new_key = 'work_exp'\n",
    "\n",
    "        elif type_ == 'language':\n",
    "            new_key = 'lang'\n",
    "\n",
    "        elif type_ == 'major':\n",
    "            new_key = 'major'\n",
    "\n",
    "        elif type_ == 'skill':\n",
    "            new_key = 'skill'\n",
    "\n",
    "        elif type_ == 'cert':\n",
    "            new_key = 'cert'\n",
    "            \n",
    "        else:\n",
    "            new_key = type_\n",
    "\n",
    "        new_analysis_dict[new_key] = {}\n",
    "        for k1, v1 in analysis_dict[type_].items():\n",
    "            if k1.isdigit():\n",
    "                for k2, v2 in v1.items():\n",
    "                    if 'Name' in k2:\n",
    "                        new_analysis_dict[new_key][v2.strip()] = int(v1['count'])\n",
    "                        break\n",
    "                        \n",
    "    return new_analysis_dict\n",
    "\n",
    "\n",
    "def _get_other_apply_count(apply_count: dict, filter_list: list) -> int:\n",
    "    other_count = 0\n",
    "    for k, v in apply_count.items():\n",
    "        if k in filter_list:\n",
    "            continue\n",
    "        other_count += v\n",
    "    return other_count\n",
    "\n",
    "\n",
    "\n",
    "_parse_analysis_content(analysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_job_detail_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    # 職缺名稱\n",
    "    parsed_job['job_name'] = crawled_dict['header']['jobName']\n",
    "    \n",
    "    # 開缺日期\n",
    "#     parsed_job['appear_date'] = int(re.sub('/', '', crawled_dict['header']['appearDate'])) # second choice\n",
    "\n",
    "    # 公司名稱\n",
    "    parsed_job['comp_name'] = crawled_dict['header']['custName']\n",
    "    \n",
    "    # 公司連結\n",
    "    try:\n",
    "        parsed_job['comp_url'] = urljoin(DOMAIN_URL, crawled_dict['header']['custUrl'])\n",
    "    except:\n",
    "        parsed_job['comp_url'] = ''\n",
    "    \n",
    "    # 分析類型\n",
    "    parsed_job['analysis_type'] = crawled_dict['header']['analysisType']\n",
    "    \n",
    "    # 分析連結\n",
    "    try:\n",
    "        parsed_job['analysis_url'] = urljoin(DOMAIN_URL, crawled_dict['header']['analysis_url'])\n",
    "    except:\n",
    "        parsed_job['analysis_url'] = ''\n",
    "        \n",
    "    # 接受身份\n",
    "    parsed_job['accept_role'] = join_dict_item_in_list(\n",
    "        crawled_dict['condition']['acceptRole']['role'], 'description') \n",
    "    \n",
    "    # 婉拒身份\n",
    "    parsed_job['accept_role'] = join_dict_item_in_list(\n",
    "        crawled_dict['condition']['acceptRole']['disRole']['disability'], 'type') \n",
    "    \n",
    "    # 工作經驗要求\n",
    "#     parsed_job['work_exp'] = int(crawled_dict['condition']['workExp']) # second choice\n",
    "    \n",
    "    # 學歷要求\n",
    "#     parsed_job['edu'] = crawled_dict['condition']['edu'] # second choice\n",
    "    \n",
    "    # 科系要求\n",
    "    parsed_job['major'] = join_element_in_list(crawled_dict['condition']['major'])\n",
    "    \n",
    "    # 語言要求\n",
    "    parsed_job['lang'] = join_dict_item_in_list(crawled_dict['condition']['language'], 'language')\n",
    "    \n",
    "    # 地方語言要求\n",
    "    parsed_job['local_lang'] = join_dict_item_in_list(crawled_dict['condition']['localLanguage'], 'language')\n",
    "    \n",
    "    # 工具要求\n",
    "    parsed_job['specialty'] = join_dict_item_in_list(crawled_dict['condition']['specialty'], 'description')\n",
    "    \n",
    "    # 技能要求\n",
    "    parsed_job['skill'] = join_dict_item_in_list(crawled_dict['condition']['skill'], 'description')\n",
    "    \n",
    "    # 證照要求\n",
    "    parsed_job['cert'] = join_element_in_list(crawled_dict['condition']['certificate'])\n",
    "    \n",
    "    # 駕照要求\n",
    "#     parsed_job['driver_license'] = join_element_in_list(crawled_dict['condition']['driverLicense'])\n",
    "    \n",
    "    # 其他要求\n",
    "    parsed_job['other'] = crawled_dict['condition']['other']\n",
    "    \n",
    "    # 職缺內容\n",
    "    parsed_job['job_detail'] = crawled_dict['jobDetail']['jobDescription']\n",
    "    \n",
    "    # 職缺分類\n",
    "    parsed_job['job_cat'] = join_dict_item_in_list(crawled_dict['jobDetail']['jobCategory'], 'description')\n",
    "    \n",
    "    # 最低薪資\n",
    "    parsed_job['salary_min'] = int(crawled_dict['jobDetail']['salaryMin'])\n",
    "    \n",
    "    # 最高薪資\n",
    "    parsed_job['salary_max'] = int(crawled_dict['jobDetail']['salaryMax'])\n",
    "    \n",
    "    # 薪資描述\n",
    "    parsed_job['salary_desc'] = crawled_dict['jobDetail']['salary']\n",
    "    \n",
    "    # 薪資類型\n",
    "    parsed_job['salary_type'] = crawled_dict['jobDetail']['salaryType']\n",
    "    \n",
    "    # 職缺類型\n",
    "    parsed_job['job_type'] = crawled_dict['jobDetail']['jobType']\n",
    "    \n",
    "    # 工作性質：全職、兼職...\n",
    "    parsed_job['job_role'] = crawled_dict['jobDetail']['workType'] # or work_type?\n",
    "    \n",
    "    # 工作地區：台北市信義區...\n",
    "    parsed_job['job_addr_dist'] = crawled_dict['jobDetail']['addressRegion']\n",
    "    \n",
    "    # 工作地點經度\n",
    "    parsed_job['lon'] = crawled_dict['jobDetail']['longitude']\n",
    "    \n",
    "    # 工作地點緯度\n",
    "    parsed_job['lat'] = crawled_dict['jobDetail']['latitude']\n",
    "    \n",
    "    # 管理責任\n",
    "    parsed_job['manage_resp'] = crawled_dict['jobDetail']['manageResp']\n",
    "    \n",
    "    # 出差外派\n",
    "    parsed_job['business_trip'] = crawled_dict['jobDetail']['businessTrip']\n",
    "    \n",
    "    # 上班時段\n",
    "    parsed_job['work_period'] = crawled_dict['jobDetail']['workPeriod']\n",
    "    \n",
    "    # 休假制度\n",
    "    parsed_job['vacation_policy'] = crawled_dict['jobDetail']['vacationPolicy']\n",
    "    \n",
    "    # 可上班日\n",
    "    parsed_job['start_work_day'] = crawled_dict['jobDetail']['startWorkingDay']\n",
    "    \n",
    "    # 招募類型：0公司自聘、1代徵\n",
    "    parsed_job['hire_type'] = crawled_dict['jobDetail']['hireType']\n",
    "    \n",
    "    # 委託招募單位\n",
    "    parsed_job['delegate_recruit'] = crawled_dict['jobDetail']['delegatedRecruit']\n",
    "    \n",
    "    # 需求人數\n",
    "    parsed_job['need_emp'] = crawled_dict['jobDetail']['needEmp']\n",
    "    \n",
    "    # 最少需求人數\n",
    "    parsed_job['need_count_min'] = _get_need_emp_min(parsed_job['need_emp'])\n",
    "    \n",
    "    # 最多需求人數\n",
    "    parsed_job['need_count_max'] = _get_need_emp_max(parsed_job['need_emp'])\n",
    "    \n",
    "    # 職缺狀況\n",
    "#     parsed_job['switch'] = crawled_dict['switch']['needEmp']\n",
    "    \n",
    "    # 產業描述\n",
    "    parsed_job['indust_desc'] = crawled_dict['industry']\n",
    "    \n",
    "    # 公司編號\n",
    "    parsed_job['comp_no'] = crawled_dict['custNo']\n",
    "    \n",
    "    return parsed_job\n",
    "   \n",
    "    \n",
    "def _get_need_emp_min(need_emp_string):\n",
    "    result = re.findall('(\\d+)', need_emp_string)\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "def _get_need_emp_max(need_emp_string):\n",
    "    result = re.findall('(\\d+)', need_emp_string)\n",
    "    if result:\n",
    "        return result[-1]\n",
    "    else:\n",
    "        return 99\n",
    "    \n",
    "_parse_job_detail_content(job_dict['data'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = 'https://www.104.com.tw/'\n",
    "\n",
    "def _parse_job_list_content(crawled_dict: dict) -> dict:\n",
    "    parsed_job = {}\n",
    "    \n",
    "    # 職缺類型\n",
    "    parsed_job['job_type'] = crawled_dict['jobType']\n",
    "    \n",
    "    # 職缺編號\n",
    "    parsed_job['job_no'] = crawled_dict['jobNo']\n",
    "    \n",
    "    # 職缺名稱\n",
    "    parsed_job['job_name'] = crawled_dict['jobName']\n",
    "    \n",
    "    # 工作性質：全職、兼職...\n",
    "    parsed_job['job_role'] = crawled_dict['jobRole']\n",
    "    \n",
    "    # 工作地區：台北市信義區...\n",
    "    parsed_job['job_addr_dist'] = crawled_dict['jobAddrNoDesc']\n",
    "    \n",
    "    # 職缺內容\n",
    "    parsed_job['job_detail'] = crawled_dict['description']\n",
    "    \n",
    "    # 學歷要求\n",
    "    parsed_job['edu'] = crawled_dict['optionEdu']\n",
    "    \n",
    "    # 工作經驗要求\n",
    "    parsed_job['work_exp'] = int(crawled_dict['period'])\n",
    "    \n",
    "    # 應徵人數\n",
    "    parsed_job['apply_count'] = int(crawled_dict['applyCnt'])\n",
    "    \n",
    "    # 公司編號\n",
    "    parsed_job['comp_no'] = crawled_dict['custNo']\n",
    "    \n",
    "    # 公司名稱\n",
    "    parsed_job['comp_name'] = crawled_dict['custName']\n",
    "    \n",
    "    # 產業編號\n",
    "    parsed_job['indust_no'] = crawled_dict['coIndustry']\n",
    "    \n",
    "    # 產業描述\n",
    "    parsed_job['indust_desc'] = crawled_dict['coIndustryDesc']\n",
    "    \n",
    "    # 最低薪資\n",
    "    parsed_job['salary_min'] = int(crawled_dict['salaryLow'])\n",
    "    \n",
    "    # 最高薪資\n",
    "    parsed_job['salary_max'] = int(crawled_dict['salaryHigh'])\n",
    "    \n",
    "    # 薪資描述\n",
    "    parsed_job['salary_desc'] = crawled_dict['salaryDesc']\n",
    "    \n",
    "    # 薪資類型\n",
    "    parsed_job['salary_type'] = crawled_dict['s10']\n",
    "    \n",
    "    # 開缺日期\n",
    "    parsed_job['appear_date'] = int(crawled_dict['appearDate'])\n",
    "    \n",
    "    # 職缺標籤\n",
    "    parsed_job['job_tag'] = crawled_dict['tags']\n",
    "    \n",
    "    # 工作地點地標\n",
    "    parsed_job['landmark_tag'] = crawled_dict['landmark']\n",
    "    \n",
    "    # 職缺連結\n",
    "    try:\n",
    "        parsed_job['job_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['job'])\n",
    "    except:\n",
    "        parsed_job['job_url'] = ''\n",
    "    \n",
    "    # 應徵分析連結\n",
    "    try:\n",
    "        parsed_job['analysis_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['applyAnalyze'])\n",
    "    except:\n",
    "        parsed_job['analysis_url'] = ''\n",
    "    \n",
    "    # 公司連結\n",
    "    try:\n",
    "        parsed_job['comp_url'] = urljoin(DOMAIN_URL, crawled_dict['link']['cust'])\n",
    "    except:\n",
    "        parsed_job['comp_url'] = ''\n",
    "    \n",
    "    # 工作地點經度\n",
    "    parsed_job['lon'] = crawled_dict['lon']\n",
    "    \n",
    "    # 工作地點緯度\n",
    "    parsed_job['lat'] = crawled_dict['lat']\n",
    "    \n",
    "    return parsed_job\n",
    "\n",
    "\n",
    "def _get_web_link(arg: str) -> str:\n",
    "    try:\n",
    "        return urljoin(DOMAIN_URL, crawled_dict['link'][arg])\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "_parse_job_list_content(search_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for k, v in search_dict.items():\n",
    "    if k in map_search_json_key.keys():\n",
    "        new_key = map_search_json_key[k]\n",
    "        d[new_key] = v\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_dict(d, return_dict={}):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            return_dict = get_flat_dict(value, return_dict)\n",
    "        else:\n",
    "            return_dict[key] = value\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
